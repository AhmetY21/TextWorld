{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple agent with TextWorld\n",
    "This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning challenges\n",
    "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
    "\n",
    "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
    "2. a really sparse reward signal.\n",
    "\n",
    "To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n",
    "\n",
    "- __Description__:\n",
    "For every game state, we will get the output of the `look` command which describes the current location;\n",
    "\n",
    "- __Inventory__:\n",
    "For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n",
    "\n",
    "- __Admissible commands__:\n",
    "For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n",
    "\n",
    "- __Intermediate reward__:\n",
    "For every game state, we will get an intermediate reward which can either be:\n",
    "  - __-1__: last action needs to be undone before resuming the quest\n",
    "  -  __0__: last action didn't affect the quest\n",
    "  -  __1__: last action brought us closer to completing the quest\n",
    "\n",
    "- __Entities__:\n",
    "For every game, we will get a list of entity names that the agent can interact with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test games\n",
    "We can use TextWorld to generate a few simple games with the following handcrafted world\n",
    "```\n",
    "                     Bathroom\n",
    "                        +\n",
    "                        |\n",
    "                        +\n",
    "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
    "      (P)               +                  +\n",
    "                        |                  |\n",
    "                        +                  +\n",
    "                   Living Room           Garden\n",
    "```\n",
    "where the goal is always to retrieve a hidden food item and put it on the stove which located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n",
    "\n",
    "Using `tw-make tw-simple ...` (see `make_games.sh` for the exact commands), we generated the following 7 games:\n",
    "\n",
    "| gamefile | description |\n",
    "| -------- | ----------- |\n",
    "| `games/rewardsDense_goalDetailed.ulx` | dense reward + detailed instructions |\n",
    "| `games/rewardsBalanced_goalDetailed.ulx` | balanced rewards + detailed instructions |\n",
    "| `games/rewardsSparse_goalDetailed.ulx` | sparse rewards + detailed instructions |\n",
    "| `games/rewardsDense_goalBrief.ulx` | dense rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsBalanced_goalBrief.ulx` | balanced rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsSparse_goalBrief.ulx` | sparse rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsSparse_goalNone.ulx` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the random baseline\n",
    "Let's start with building an agent that simply selects an admissible command at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "\n",
    "\n",
    "class RandomAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        return self.rng.choice(infos[\"admissible_commands\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play function\n",
    "Let's write a simple function to play a text-based game using an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import gym\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "def play(agent, path, max_step=50, nb_episodes=10, verbose=True):\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.ulx\"))\n",
    "        \n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "        \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            nb_moves += 1\n",
    "        \n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "                \n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  4.2 / 11.\n",
      "rewardsBalanced_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.7 / 5.\n",
      "rewardsSparse_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/rewardsDense_goalDetailed.ulx\")    # Dense rewards\n",
    "play(RandomAgent(), \"./games/rewardsBalanced_goalDetailed.ulx\") # Balanced rewards\n",
    "play(RandomAgent(), \"./games/rewardsSparse_goalDetailed.ulx\")   # Sparse rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural agent\n",
    "\n",
    "Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description.\n",
    "\n",
    "Here's the implementation of that learning agent that uses [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CommandScorer, self).__init__()\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic       = nn.Linear(hidden_size, 1)\n",
    "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, obs, commands, **kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "\n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "\n",
    "        # Attention network over the commands.\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
    "\n",
    "        # Same observed state for all commands.\n",
    "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Same command choices for the whole batch.\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Concatenate the observed state and command encodings.\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
    "\n",
    "        # Compute one score per command.\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
    "\n",
    "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
    "        return scores, index, value\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class NeuralAgent:\n",
    "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
    "    MAX_VOCAB_SIZE = 1000\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 1000\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        has_won=True, has_lost=True)\n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "            \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    def _process(self, texts):\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text)] = text\n",
    "\n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "      \n",
    "    def _discount_rewards(self, last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
    "        \n",
    "        # Tokenize and pad the input and the commands to chose from.\n",
    "        input_tensor = self._process([input_])\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
    "        \n",
    "        # Get our next action and value prediction.\n",
    "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"has_won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"has_lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {:.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
    "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
    "        \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the neural agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps:  95.4; avg. score:  4.5 / 11.\n"
     ]
    }
   ],
   "source": [
    "agent = NeuralAgent()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the result is not much different from what the random agent can get.\n",
    "\n",
    "Let's train the agent for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "1000. reward: -0.054  policy: 0.134  value: 5.593  entropy: 2.381  confidence: 0.096  score: 10  vocab: 327\n",
      "2000. reward: 0.037  policy: 0.027  value: 0.056  entropy: 2.367  confidence: 0.096  score: 6  vocab: 331\n",
      "3000. reward: 0.050  policy: 0.125  value: 0.095  entropy: 2.396  confidence: 0.094  score: 10  vocab: 331\n",
      "4000. reward: 0.042  policy: 0.059  value: 0.074  entropy: 2.357  confidence: 0.099  score: 7  vocab: 331\n",
      "5000. reward: -0.046  policy: -0.118  value: 5.368  entropy: 2.476  confidence: 0.088  score: 10  vocab: 331\n",
      "6000. reward: 0.056  policy: 0.021  value: 0.118  entropy: 2.447  confidence: 0.092  score: 7  vocab: 331\n",
      "7000. reward: 0.060  policy: 0.056  value: 0.110  entropy: 2.466  confidence: 0.091  score: 10  vocab: 331\n",
      "8000. reward: 0.062  policy: 0.036  value: 0.097  entropy: 2.467  confidence: 0.094  score: 8  vocab: 331\n",
      "9000. reward: 0.053  policy: 0.019  value: 0.082  entropy: 2.413  confidence: 0.098  score: 8  vocab: 331\n",
      "10000. reward: 0.048  policy: -0.027  value: 0.090  entropy: 2.439  confidence: 0.095  score: 6  vocab: 331\n",
      "11000. reward: 0.057  policy: 0.046  value: 0.092  entropy: 2.361  confidence: 0.103  score: 10  vocab: 331\n",
      "12000. reward: 0.059  policy: 0.104  value: 0.098  entropy: 2.413  confidence: 0.097  score: 10  vocab: 331\n",
      "13000. reward: 0.058  policy: 0.025  value: 0.111  entropy: 2.452  confidence: 0.095  score: 10  vocab: 331\n",
      "14000. reward: 0.063  policy: -0.015  value: 0.104  entropy: 2.368  confidence: 0.102  score: 10  vocab: 331\n",
      "15000. reward: 0.057  policy: 0.018  value: 0.116  entropy: 2.422  confidence: 0.098  score: 8  vocab: 331\n",
      "16000. reward: 0.060  policy: -0.041  value: 0.087  entropy: 2.413  confidence: 0.099  score: 7  vocab: 331\n",
      "17000. reward: 0.057  policy: -0.019  value: 0.098  entropy: 2.380  confidence: 0.106  score: 10  vocab: 331\n",
      "18000. reward: -0.038  policy: -1.165  value: 24.336  entropy: 2.520  confidence: 0.092  score: 10  vocab: 331\n",
      "19000. reward: -0.039  policy: -1.112  value: 18.672  entropy: 2.447  confidence: 0.099  score: 10  vocab: 332\n",
      "20000. reward: 0.067  policy: 0.063  value: 0.134  entropy: 2.420  confidence: 0.101  score: 8  vocab: 332\n",
      "21000. reward: 0.067  policy: 0.029  value: 0.146  entropy: 2.458  confidence: 0.099  score: 10  vocab: 332\n",
      "22000. reward: -0.143  policy: -1.543  value: 27.042  entropy: 2.481  confidence: 0.101  score: 10  vocab: 334\n",
      "23000. reward: -0.136  policy: -2.391  value: 40.653  entropy: 2.461  confidence: 0.101  score: 10  vocab: 336\n",
      "24000. reward: -0.017  policy: -1.120  value: 20.916  entropy: 2.484  confidence: 0.101  score: 10  vocab: 337\n",
      "25000. reward: -0.011  policy: -0.538  value: 13.134  entropy: 2.451  confidence: 0.113  score: 10  vocab: 338\n",
      "26000. reward: -0.640  policy: -7.593  value: 130.459  entropy: 2.399  confidence: 0.117  score: 10  vocab: 345\n",
      "27000. reward: -0.322  policy: -4.633  value: 73.999  entropy: 2.359  confidence: 0.120  score: 10  vocab: 349\n",
      "28000. reward: -0.539  policy: -4.874  value: 82.080  entropy: 2.321  confidence: 0.135  score: 10  vocab: 354\n",
      "29000. reward: -0.316  policy: -3.559  value: 61.794  entropy: 2.280  confidence: 0.139  score: 10  vocab: 357\n",
      "30000. reward: -0.116  policy: -2.694  value: 43.905  entropy: 2.343  confidence: 0.131  score: 10  vocab: 358\n",
      "31000. reward: -0.311  policy: -4.234  value: 115.202  entropy: 2.322  confidence: 0.144  score: 11  vocab: 363\n",
      "32000. reward: -0.417  policy: -4.397  value: 142.462  entropy: 2.295  confidence: 0.146  score: 11  vocab: 368\n",
      "33000. reward: -0.526  policy: -7.199  value: 127.229  entropy: 2.239  confidence: 0.161  score: 10  vocab: 373\n",
      "34000. reward: -0.608  policy: -6.671  value: 165.878  entropy: 2.155  confidence: 0.178  score: 11  vocab: 379\n",
      "35000. reward: -0.432  policy: -3.810  value: 66.289  entropy: 2.259  confidence: 0.163  score: 10  vocab: 381\n",
      "36000. reward: 0.009  policy: -0.086  value: 5.679  entropy: 2.174  confidence: 0.179  score: 10  vocab: 382\n",
      "37000. reward: -0.744  policy: -7.037  value: 116.738  entropy: 2.196  confidence: 0.169  score: 10  vocab: 385\n",
      "38000. reward: -0.754  policy: -6.664  value: 108.566  entropy: 2.238  confidence: 0.164  score: 10  vocab: 386\n",
      "39000. reward: -0.723  policy: -8.868  value: 142.146  entropy: 2.176  confidence: 0.179  score: 10  vocab: 388\n",
      "40000. reward: -0.824  policy: -10.297  value: 214.200  entropy: 2.164  confidence: 0.183  score: 11  vocab: 390\n",
      "41000. reward: -0.428  policy: -2.822  value: 50.070  entropy: 2.208  confidence: 0.165  score: 10  vocab: 391\n",
      "42000. reward: -0.102  policy: -1.616  value: 32.140  entropy: 2.130  confidence: 0.179  score: 10  vocab: 391\n",
      "43000. reward: -0.313  policy: -3.777  value: 65.152  entropy: 2.156  confidence: 0.179  score: 10  vocab: 394\n",
      "44000. reward: -0.098  policy: -1.776  value: 30.463  entropy: 2.158  confidence: 0.190  score: 10  vocab: 394\n",
      "45000. reward: -0.411  policy: -6.030  value: 91.995  entropy: 2.242  confidence: 0.178  score: 10  vocab: 396\n",
      "Trained in 3079.04 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps:  82.4; avg. score: 10.1 / 11.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n",
    "\n",
    "To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.ulx` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.ulx` is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 1\n",
      "Game generated: games/another_game.ulx\n",
      "I hope you're ready to go into rooms and interact with objects, because you've just entered TextWorld! Here is how to play! First thing I need you to do is to open the antique trunk. And then, take the old key from the antique trunk within the bedroom. Once you have got the old key, make it so that the wooden door is unlocked. And then, ensure that the wooden door within the bedroom is open. And then, make an attempt to go east. Once you manage that, head south. Once you get around to doing that, pick up the milk from the couch. After that, attempt to move north. And then, put the milk on the stove. Got that? Good!\n"
     ]
    }
   ],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/another_game.ulx -v -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_game.ulx..........  \tavg. steps:  50.0; avg. score:  2.8 / 9.\n",
      "another_game.ulx..........  \tavg. steps:  47.3; avg. score:  6.7 / 9.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/another_game.ulx\")\n",
    "play(agent, \"./games/another_game.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the trained agent barely does better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n",
    "\n",
    "One could use the following command to easily generate 100 training games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 4\n",
      "Global seed: 2\n",
      "Global seed: 1\n",
      "Global seed: 3\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-E5eLHkaXFk6BSgR1.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-ek06H8B7uqoYFVEy.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-D8gMTlO8cPoEtgZx.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-ekDZtbGXIbO5FKp8.ulx\n",
      "Global seed: 6\n",
      "Global seed: 5\n",
      "Global seed: 7\n",
      "Global seed: 8\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-o2RVTmrEi6R5T3p0.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-7KpYUDDdckE0cBqZ.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-68kvf8x7TBd9Iq0P.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-KJODI168SvJVFM9x.ulx\n",
      "Global seed: 9\n",
      "Global seed: 11\n",
      "Global seed: 10\n",
      "Global seed: 12\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-NPQ8TkJ9i2x6fYDM.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Q9nDu630U5j3tqBG.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-redEHVr6CmKYhrJg.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Qbq3h3VWFkPdtogB.ulx\n",
      "Global seed: 13\n",
      "Global seed: 14\n",
      "Global seed: 16\n",
      "Global seed: 15\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1QKVfg6YhRb1ul1e.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-jROVIEqEIya6Tr0L.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6GMVtjVYF5QRupyN.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-7yGrcV9pTE8DF75n.ulx\n",
      "Global seed: 18\n",
      "Global seed: 17\n",
      "Global seed: 19\n",
      "Global seed: 20\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-nrEoHEqgUba7U1nx.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Mn8oTkr2fvv8TX1.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-VLpEiW2msKJpTZ7J.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-o8WVtobyuRR0Uqv5.ulx\n",
      "Global seed: 22\n",
      "Global seed: 21\n",
      "Global seed: 23\n",
      "Global seed: 24\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-JGJdTBvVHrpBiVy5.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-M7MmCGG5i6kES1kV.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6GZ3CqJvCX9rfev3.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1MB8fmosEL9HNEv.ulx\n",
      "Global seed: 25\n",
      "Global seed: 26\n",
      "Global seed: 27\n",
      "Global seed: 28\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-mn3JuMrnfPNNI5K5.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-pGedtxVJsYDxsGaV.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-P6RdC912uMrbcXBX.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-R2bxU9dJUK0LCxk0.ulx\n",
      "Global seed: 29\n",
      "Global seed: 30\n",
      "Global seed: 31\n",
      "Global seed: 32\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-W1qJibX5FqLRS3kL.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6WNBiZyofr8mu6MG.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-eDi7Z2iEJ7FdL9.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-QlDlS6DxCMnVu81D.ulx\n",
      "Global seed: 33\n",
      "Global seed: 34\n",
      "Global seed: 35\n",
      "Global seed: 36\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-5rjrFkZEiyOIj1y.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-dZ2oU2KnflPksnEY.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-KrNpTrdMtdqVUKOB.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-J9ylcQmmc190Cgn3.ulx\n",
      "Global seed: 37\n",
      "Global seed: 38\n",
      "Global seed: 39\n",
      "Global seed: 40\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-M8xnUOBkulX7HNQX.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-535aCDxgu5MqF7R1.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-nOVQCNlrINVxIDje.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-mDjNimx7S5a5tmZ7.ulx\n",
      "Global seed: 41\n",
      "Global seed: 42\n",
      "Global seed: 43\n",
      "Global seed: 44\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-566vUvgjfXgU9GK.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-67XKC3EyH2riOk8.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-El9oUXJWIYVgF1jy.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-mbJLuQBoCbbkUv1n.ulx\n",
      "Global seed: 46\n",
      "Global seed: 45\n",
      "Global seed: 47\n",
      "Global seed: 48\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-mNB2tM2ohGmRIB2J.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-M2vecROLFVBxHBvl.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-JBP7TQZ6fV7biZ9q.ulx\n",
      "Global seed: 49\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-aernSYY1f7rrflvB.ulx\n",
      "Global seed: 50\n",
      "Global seed: 51\n",
      "Global seed: 52\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Oq9ns8K0uK5EhJvr.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-5VVVSK6fGjlspnj.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Mj8NTxYWso7LfmN9.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-xqa9SlmxsDmVs8yb.ulx\n",
      "Global seed: 53\n",
      "Global seed: 54\n",
      "Global seed: 55\n",
      "Global seed: 56\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-EloyS7BZs8LRHNVn.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-GN1YHrQ6s6vBTnNb.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-XR5RUnmMIgOnCYyy.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-p1RLFX7MU7KjFy0d.ulx\n",
      "Global seed: 57\n",
      "Global seed: 58\n",
      "Global seed: 59\n",
      "Global seed: 60\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-WxnkhG07upKRhbNV.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Kx32iybbtDGRFQl6.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-DWvMiBQvCR5sNkx.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-R7y2UJkBUeEqS8Bk.ulx\n",
      "Global seed: 61\n",
      "Global seed: 62\n",
      "Global seed: 63\n",
      "Global seed: 64\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-YnvLs6vYIWvWC51e.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1EQPheDOiVB8I7m5.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6VXXCVGKiEWkhPer.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-G6a7HeZPcJNGhLQ2.ulx\n",
      "Global seed: 65\n",
      "Global seed: 66\n",
      "Global seed: 67\n",
      "Global seed: 68\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1jr7FeK9TgZCQ9n.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-0a72tMGVtQnPSvMR.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-2OOjSlaNUBKcgLy.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1vbrCekqT6xMcMdP.ulx\n",
      "Global seed: 69\n",
      "Global seed: 70\n",
      "Global seed: 71\n",
      "Global seed: 72\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6kRniM7gfXQNCyj6.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-kGDLudo8fXeJH1Yg.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-RvQYhbdKfMyqS5Wp.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-aMq9U18aFMlOHVEe.ulx\n",
      "Global seed: 73\n",
      "Global seed: 74\n",
      "Global seed: 75\n",
      "Global seed: 76\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-8koWh6KqcgNPI8Ox.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-165ai83atZWatMRa.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-Eykru3m8ilOGiBnx.ulx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-vKm0IdN8c0VRfJl1.ulx\n",
      "Global seed: 77\n",
      "Global seed: 78\n",
      "Global seed: 79\n",
      "Global seed: 80\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-gPyQi5vkhdQOUV0J.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-D3PEFjpNIQykuqra.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-jYPJukgls6oZf6qG.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-eRNBC9q7tr5Vi3Mv.ulx\n",
      "Global seed: 81\n",
      "Global seed: 82\n",
      "Global seed: 83\n",
      "Global seed: 84\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-0lvotxEKtYZ6umyJ.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-3bj9creDs3V0IKKN.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-qZDfWDNTJmQiLvq.ulx\n",
      "Global seed: 85\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-6yPJsEJMTeR6fpPR.ulx\n",
      "Global seed: 86\n",
      "Global seed: 87\n",
      "Global seed: 88\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-WrmvTdgGc05rh3eZ.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-g0GEcMQ5CWaeF3nJ.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-5PBeh9J3IX5XUjL3.ulx\n",
      "Global seed: 89\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-DvPBs6OphW6nCnQ3.ulx\n",
      "Global seed: 90\n",
      "Global seed: 91\n",
      "Global seed: 92\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-eBxGu3KeiYjbHDl9.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-XElPhYRjT8gWs1eM.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-3qLvIOZ9c66Igby.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-OWRZsvM8h6XyIpQy.ulx\n",
      "Global seed: 93\n",
      "Global seed: 94\n",
      "Global seed: 95\n",
      "Global seed: 96\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-gkdXTMLPtKpeSY5J.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-n3OYtnV7IY8rTkBq.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-q3rdiVK5I1lrsNpj.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-WvgBsoOmhDObfRgL.ulx\n",
      "Global seed: 97\n",
      "Global seed: 98\n",
      "Global seed: 99\n",
      "Global seed: 100\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-xQNoFqlDs10nfr79.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-1dgPTd1ki5kQUkrn.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-o7PNc5j0URa5HYLb.ulx\n",
      "Game generated: training_games/tw-simple-rDense+gDetailed+train-house-GP-2KK5iXD8ueb3ikl6.ulx\n"
     ]
    }
   ],
   "source": [
    "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --output training_games/ --seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train our agent on that set of training games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 100 games\n",
      "1000. reward: 0.064  policy: 0.417  value: 0.122  entropy: 2.278  confidence: 0.105  score: 5  vocab: 495\n",
      "2000. reward: 0.064  policy: 0.203  value: 0.113  entropy: 2.312  confidence: 0.101  score: 6  vocab: 552\n",
      "3000. reward: 0.064  policy: 0.064  value: 0.089  entropy: 2.331  confidence: 0.099  score: 5  vocab: 586\n",
      "4000. reward: 0.066  policy: 0.093  value: 0.094  entropy: 2.300  confidence: 0.104  score: 5  vocab: 602\n",
      "5000. reward: 0.071  policy: 0.040  value: 0.116  entropy: 2.331  confidence: 0.104  score: 6  vocab: 623\n",
      "6000. reward: -0.018  policy: -0.695  value: 13.661  entropy: 2.383  confidence: 0.101  score: 8  vocab: 646\n",
      "7000. reward: 0.084  policy: 0.052  value: 0.139  entropy: 2.345  confidence: 0.109  score: 6  vocab: 654\n",
      "8000. reward: 0.080  policy: -0.076  value: 0.110  entropy: 2.349  confidence: 0.108  score: 5  vocab: 654\n",
      "9000. reward: 0.089  policy: 0.056  value: 0.152  entropy: 2.359  confidence: 0.108  score: 5  vocab: 660\n",
      "10000. reward: 0.092  policy: 0.056  value: 0.148  entropy: 2.366  confidence: 0.111  score: 6  vocab: 668\n",
      "11000. reward: 0.083  policy: -0.057  value: 0.147  entropy: 2.273  confidence: 0.118  score: 5  vocab: 672\n",
      "12000. reward: 0.108  policy: 0.150  value: 0.154  entropy: 2.356  confidence: 0.111  score: 10  vocab: 674\n",
      "13000. reward: 0.092  policy: -0.019  value: 0.138  entropy: 2.361  confidence: 0.110  score: 5  vocab: 676\n",
      "14000. reward: 0.080  policy: -0.165  value: 0.128  entropy: 2.306  confidence: 0.116  score: 6  vocab: 676\n",
      "15000. reward: 0.094  policy: 0.050  value: 0.151  entropy: 2.331  confidence: 0.114  score: 6  vocab: 676\n",
      "16000. reward: -0.014  policy: -1.542  value: 23.149  entropy: 2.329  confidence: 0.117  score: 8  vocab: 685\n",
      "17000. reward: 0.092  policy: 0.008  value: 0.158  entropy: 2.358  confidence: 0.117  score: 6  vocab: 686\n",
      "18000. reward: 0.101  policy: 0.062  value: 0.172  entropy: 2.381  confidence: 0.115  score: 8  vocab: 688\n",
      "19000. reward: 0.093  policy: -0.116  value: 0.158  entropy: 2.348  confidence: 0.117  score: 6  vocab: 688\n",
      "20000. reward: 0.107  policy: 0.022  value: 0.169  entropy: 2.396  confidence: 0.108  score: 6  vocab: 693\n",
      "21000. reward: 0.114  policy: 0.014  value: 0.181  entropy: 2.417  confidence: 0.116  score: 7  vocab: 697\n",
      "22000. reward: -0.091  policy: -2.832  value: 42.840  entropy: 2.381  confidence: 0.123  score: 10  vocab: 702\n",
      "23000. reward: 0.018  policy: -1.261  value: 20.541  entropy: 2.372  confidence: 0.130  score: 10  vocab: 709\n",
      "24000. reward: 0.249  policy: 0.884  value: 11.226  entropy: 2.444  confidence: 0.124  score: 10  vocab: 711\n",
      "25000. reward: 0.264  policy: 2.264  value: 25.591  entropy: 2.353  confidence: 0.140  score: 11  vocab: 716\n",
      "Trained in 1559.23 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training on 100 games\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the agent on the test distribution\n",
    "We will generate 20 test games and evaluate the agent on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 4\n",
      "Global seed: 1\n",
      "Global seed: 3\n",
      "Global seed: 2\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-ekDZtbGXIbO5FKp8.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-E5eLHkaXFk6BSgR1.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-ek06H8B7uqoYFVEy.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-D8gMTlO8cPoEtgZx.ulx\n",
      "Global seed: 5\n",
      "Global seed: 6\n",
      "Global seed: 7\n",
      "Global seed: 8\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-7KpYUDDdckE0cBqZ.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-o2RVTmrEi6R5T3p0.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-68kvf8x7TBd9Iq0P.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-KJODI168SvJVFM9x.ulx\n",
      "Global seed: 11\n",
      "Global seed: 9\n",
      "Global seed: 10\n",
      "Global seed: 12\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-NPQ8TkJ9i2x6fYDM.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-redEHVr6CmKYhrJg.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-Q9nDu630U5j3tqBG.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-Qbq3h3VWFkPdtogB.ulx\n",
      "Global seed: 15\n",
      "Global seed: 13\n",
      "Global seed: 14\n",
      "Global seed: 16\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-1QKVfg6YhRb1ul1e.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-jROVIEqEIya6Tr0L.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-7yGrcV9pTE8DF75n.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-6GMVtjVYF5QRupyN.ulx\n",
      "Global seed: 17\n",
      "Global seed: 19\n",
      "Global seed: 18\n",
      "Global seed: 20\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-nrEoHEqgUba7U1nx.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-VLpEiW2msKJpTZ7J.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-Mn8oTkr2fvv8TX1.ulx\n",
      "Game generated: testing_games/tw-simple-rDense+gDetailed+test-house-GP-o8WVtobyuRR0Uqv5.ulx\n"
     ]
    }
   ],
   "source": [
    "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --output testing_games/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps:  47.4; avg. score:  8.3 / 11.\n",
      "./testing_games........................................................................................................................................................................................................  \tavg. steps:  49.2; avg. score:  0.7 / 1.\n",
      "./testing_games........................................................................................................................................................................................................  \tavg. steps:  50.0; avg. score:  0.3 / 1.\n"
     ]
    }
   ],
   "source": [
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game.\n",
    "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not being perfect, the agent manage to score more points on average compared to the random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Here are a few possible directions one can take to improve the agent's performance.\n",
    "- Adding more training games\n",
    "- Changing the agent architecture\n",
    "- Leveraging already trained word embeddings\n",
    "- Playing more games at once (see [`textworld.gym.make_batch`](https://textworld.readthedocs.io/en/latest/textworld.gym.html#textworld.gym.utils.make_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Papers about RL applied to text-based games\n",
    "* [Language Understanding for Text-based games using Deep Reinforcement Learning][narasimhan_et_al_2015]\n",
    "* [Learning How Not to Act in Text-based Games][haroush_et_al_2017]\n",
    "* [Deep Reinforcement Learning with a Natural Language Action Space][he_et_al_2015]\n",
    "* [What can you do with a rock? Affordance extraction via word embeddings][fulda_et_al_2017]\n",
    "* [Text-based adventures of the Golovin AI Agent][kostka_et_al_2017]\n",
    "* [Using reinforcement learning to learn how to play text-based games][zelinka_2018]\n",
    "\n",
    "[narasimhan_et_al_2015]: https://arxiv.org/abs/1506.08941\n",
    "[haroush_et_al_2017]: https://openreview.net/pdf?id=B1-tVX1Pz\n",
    "[he_et_al_2015]: https://arxiv.org/abs/1511.04636\n",
    "[fulda_et_al_2017]: https://arxiv.org/abs/1703.03429\n",
    "[kostka_et_al_2017]: https://arxiv.org/abs/1705.05637\n",
    "[zelinka_2018]: https://arxiv.org/abs/1801.01999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
